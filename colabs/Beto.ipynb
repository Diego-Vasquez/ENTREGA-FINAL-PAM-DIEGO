{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Beto.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"X-t-lqgv0aGF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#cd drive/MyDrive/"],"metadata":{"id":"qUGGOkob6-Un"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd drive/MyDrive/univer/Expociencia/Code/Beto"],"metadata":{"id":"bXweLzBxdm40"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vFu4CiVH61Pu"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from transformers import BertTokenizerFast, BertModel\n","from torch.utils.data import Dataset, DataLoader\n","from torch import nn, optim\n","import torch\n","from sklearn.metrics import f1_score\n","import time\n","from sklearn.metrics import accuracy_score, mean_absolute_error\n","from pathlib import Path\n","from torch.nn import functional as F\n","from tqdm.auto import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"Hxay8avh7RGH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["name_dataset = \"yelp_clean\"\n","#train_data = pd.read_csv('train_yelp_clean.csv')\n","#val_data = pd.read_csv('val_yelp_clean.csv')\n","test_data = pd.read_csv('test_yelp_clean.csv')"],"metadata":{"id":"QqODbVXz8HRP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train_data['Score'] = train_data['Score'].apply(lambda x: x-1) \n","#val_data['Score'] = val_data['Score'].apply(lambda x: x-1) \n","test_data['Score'] = test_data['Score'].apply(lambda x: x-1)"],"metadata":{"id":"ggTOE6m_GBq_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_LEN = 128 \n","BATCH_SIZE = 32\n","NCLASSES= 5\n","DropOut = 0.1\n","EPOCHS= 1\n","LEARNING_RATE=2e-5\n","is_with_pretrain = False \n","NAME_BERT_MODEL=\"dccuchile/bert-base-spanish-wwm-cased\"\n","#NAME=\"bert-base-spanish-wwm-cased\"+\"_\"+name_dataset\n","#if is_with_pretrain:\n","#    NAME+=\"_with_pretrain\"+\"_\"+name_dataset\n","#myfile = Path(f'{NAME} LR {LEARNING_RATE}.txt')\n","#myfile.touch(exist_ok=True)\n","BERT = BertModel.from_pretrained(NAME_BERT_MODEL)\n","RANDOM_SEED =  42\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"id":"xcFgrkhQ8o5-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizerFast.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")"],"metadata":{"id":"GIzQjw399c_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#if is_with_pretrain:\n","#    loaded_checkpoint = torch.load('Pretrined_NewBERT_CLEAN_weightModelBERT.pth', map_location=device)\n","#    BERT.load_state_dict(loaded_checkpoint['weight_model'])\n","#    print('ok')"],"metadata":{"id":"7Egyj1JC1o-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LoadDataset(Dataset):\n","    def __init__(self, reviews, labels, tokenizer, max_len, mode_truncation):\n","        self.reviews = reviews\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.mode_truncation = mode_truncation\n","\n","    def __len__(self):\n","        return len(self.reviews)\n","    \n","    def __getitem__(self, item):\n","        review = str(self.reviews[item]) \n","        label = self.labels[item]\n","        tokens = self.tokenizer.tokenize(review) \n","        tokens = ['[CLS]'] + tokens + ['[SEP]']\n","        if len(tokens) < self.max_len:\n","            tokens = tokens + ['[PAD]' for item in range(self.max_len-len(tokens))]\n","        elif len(tokens) > self.max_len:\n","            if self.mode_truncation == \"head\":\n","                tokens = tokens[:self.max_len-1] + ['[SEP]']    \n","            elif self.mode_truncation == \"head+tail\":\n","                tokens = tokens[:int(self.max_len/2)+1] + tokens[int((-1)*self.max_len/2)+1:]\n","                #tokens = ['[CLS]'] + tokens + ['[SEP]']\n","            elif self.mode_truncation == \"tail\":\n","                tokens = ['[CLS]'] + tokens[-int((self.max_len)/2):]\n","        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n","        tokens_ids_tensor = torch.tensor(token_ids)\n","        attn_mask = (tokens_ids_tensor != 1).long()\n","        return{\n","            'review':review,\n","            'input_ids':tokens_ids_tensor.flatten(),\n","            'attention_mask':attn_mask.flatten(),\n","            'label':torch.tensor(label, dtype=torch.long)}\n","\n","def data_loader(df, tokenizer, max_len, batch_size, modo):\n","  dataset = LoadDataset(\n","    reviews=df.Text.to_numpy(),\n","    labels = df.Score.to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=max_len,\n","    mode_truncation = modo\n","  )\n","  return DataLoader(dataset, batch_size= BATCH_SIZE, num_workers=2)"],"metadata":{"id":"Z9kQBq9f9g_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train_data_loader = data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE, \"head+tail\")\n","#validation_data_loader = data_loader(val_data, tokenizer, MAX_LEN, BATCH_SIZE, \"head+tail\")\n","test_data_loader = data_loader(test_data, tokenizer, MAX_LEN, BATCH_SIZE, \"head+tail\")"],"metadata":{"id":"Nh8TSaYl97q8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERTModel(nn.Module):\n","    def __init__(self, n_class):\n","        super(BERTModel, self).__init__()\n","        self.bert = BERT\n","        self.do = nn.Dropout(DropOut)\n","        self.linear = nn.Linear(self.bert.config.hidden_size, n_class)\n","        #self.softmax = torch.nn.Softmax(dim=1)\n","        nn.init.xavier_uniform_(self.linear.weight)\n","    def forward(self, input_ids, attention_mask):\n","        _, cls_output = self.bert(\n","            input_ids = input_ids,\n","            attention_mask = attention_mask,\n","            return_dict=False\n","        )\n","        dropout = self.do(cls_output)\n","        output = self.linear(dropout)\n","        return output"],"metadata":{"id":"0-cRNanm-9GB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BERTModel(NCLASSES)\n","model.to(device)\n","print('ok')"],"metadata":{"id":"aZOEkWOr_WgF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_fn = nn.CrossEntropyLoss().to(device)\n","#optimizer = torch.optim.AdamW(model.parameters(),  eps=1e-6, betas=(0.9, 0.99), weight_decay=0.01, lr=LEARNING_RATE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","\"\"\"total_steps = len(train_data_loader)*EPOCHS\n","\n","scheduler = get_linear_schedule_with_warmup(\n","  optimizer,\n","  num_warmup_steps=10000,\n","  num_training_steps = total_steps\n",")\"\"\""],"metadata":{"id":"dILhi5yk_Yiy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#int(len(train_data)/BATCH_SIZE)"],"metadata":{"id":"N4LDCq68f9r9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model (model, data_loader, loss_fn, optimizer, device, epoch, n_examples):\n","  model=model.train()\n","  losses = []\n","  accuracy_global = []\n","  f1_score_global = []\n","  f1_weight_global = []\n","\n","  i = 0\n","  for batch in data_loader:\n","    input_ids = batch['input_ids'].to(device)\n","    attention_mask = batch['attention_mask'].to(device)\n","    labels = batch['label'].to(device)\n","    outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n","    _, preds = torch.max(outputs, dim=1)\n","    loss = loss_fn(outputs, labels)\n","    f1_score_global.append(np.mean(f1_score(preds.cpu().clone().detach().numpy(), labels.cpu().clone().detach().numpy(), average=None)))\n","    f1_weight_global.append(np.mean(f1_score(preds.cpu().clone().detach().numpy(), labels.cpu().clone().detach().numpy(), average='weighted')))\n","    ac = accuracy_score(preds.cpu().clone().detach().numpy(), labels.cpu().clone().detach().numpy())\n","    accuracy_global.append(ac)\n","    losses.append(loss.item())\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    #scheduler.step()\n","    optimizer.zero_grad()\n","    if i%5==0:\n","        print('Ejemplo {}/{} , Entrenamiento: Loss: {}, accuracy: {} -- f1_score: {} -- f1 weight: {} '.format(i, int(n_examples/BATCH_SIZE),\n","                                                                                               np.mean(losses), np.mean(accuracy_global), np.mean(f1_score_global), \n","                                                                                                   np.mean(f1_weight_global)))\n","    if (i%1000==0 and i>9999) or (i == int(n_examples/BATCH_SIZE)-1):\n","        validation_acc, validation_loss, validation_f1, val_weight, history_val = eval_model(\n","            model, validation_data_loader, loss_fn, device, len(val_data), 'Validación'\n","        )\n","        print('Entrenamiento: Loss: {}, accuracy: {} -- f1_score: {} -- f1 weight: {} '.format(np.mean(losses), np.mean(accuracy_global), np.mean(f1_score_global), \n","                                                                                                              np.mean(f1_weight_global)))\n","        print('Validación: Loss: {}, accuracy: {} -- f1_score: {} -- f1 weight: {}'.format(validation_loss, validation_acc, validation_f1,\n","                                                                                                          val_weight, \n","                                                                                                          ))\n","        weight = {f'weight_model':model.state_dict()}\n","        torch.save(weight, f'epoch_{epoch}_bert_{i}_weightModelBERT.pth')\n","\n","        with open (f'{NAME} LR {LEARNING_RATE}.txt','a') as f:\n","                f.write(f'''    name: {NAME_BERT_MODEL}\n","                Iteracion : {i}\n","                Train:\n","                   -loss: {np.mean(losses)}, \n","                   Accuracy: mean: {np.mean(accuracy_global)}, std: {np.std(accuracy_global)}, var: {np.std(accuracy_global)**2}  \n","                   f1_score: {np.mean(f1_score_global)}, std: {np.std(f1_score_global)}, var: {np.std(f1_score_global)**2}\n","                   f1_weight: {np.mean(f1_weight_global)}, std: {np.std(f1_weight_global)}, var: {np.std(f1_weight_global)**2}\n","\n","                Validation\n","                   -loss: {validation_loss}, \n","                   Accuracy: mean: {validation_acc}, std: {np.std(history_val['accuracy'])}, var: {np.std(history_val['accuracy'])**2}  \n","                   f1_score: {validation_f1}, std: {np.std(history_val['f1_score'])}, var: {np.std(history_val['f1_score'])**2}\n","                   f1_weight: {val_weight}, std: {np.std(history_val['f1_weight'])}, var: {np.std(history_val['f1_weight'])**2}\n","\n","                ---------------------------------    \n","    ''')\n","    i+=1\n","  return np.mean(accuracy_global), np.mean(losses), np.mean(f1_score_global), np.mean(f1_weight_global),    {'loss':losses, \n","                                                                                                            'accuracy': accuracy_global,\n","                                                                                                            'f1_score': f1_score_global,\n","                                                                                                            'f1_weight':f1_weight_global}"],"metadata":{"id":"h-9V3FsO_uxw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval_model(model, data_loader, loss_fn, device, n_examples, modo):\n","  model = model.eval()\n","  losses = []\n","  accuracy_global = []\n","  f1_score_global = []\n","  f1_weigh_global = []\n","  i=0 \n","  with torch.no_grad():\n","    for batch in data_loader:\n","      input_ids = batch['input_ids'].to(device)\n","      attention_mask = batch['attention_mask'].to(device)\n","      labels = batch['label'].to(device)\n","      outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n","      _, preds = torch.max(outputs, dim=1)\n","      loss = loss_fn(outputs, labels)\n","      f1_score_global.append(np.mean(f1_score(preds.cpu().clone().detach().numpy(), labels.cpu().clone().detach().numpy(), average=None)))\n","      f1_weigh_global.append(np.mean(f1_score(preds.cpu().clone().detach().numpy(), labels.cpu().clone().detach().numpy(), average='weighted')))\n","      ac = accuracy_score(preds.cpu().clone().detach().numpy(), labels.cpu().clone().detach().numpy())\n","      accuracy_global.append(ac)\n","      losses.append(loss.item())\n","      if i%5==0:\n","        print('Ejemplo {}/{} , {}: Loss: {}, accuracy: {} -- f1_score: {} -- f1 weight: {}'.format(i, int(n_examples/BATCH_SIZE),\n","                                                                                               modo, np.mean(losses), np.mean(accuracy_global), np.mean(f1_score_global), \n","                                                                                               np.mean(f1_weigh_global)))\n","      i+=1\n","  return np.mean(accuracy_global), np.mean(losses), np.mean(f1_score_global), np.mean(f1_weigh_global), {'loss':losses, \n","                                                                                                        'accuracy': accuracy_global,\n","                                                                                                        'f1_score': f1_score_global,\n","                                                                                                    'f1_weight': f1_weigh_global}"],"metadata":{"id":"7-yKQmDLAA72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f'El modelo tiene {count_parameters(model)} de parámetros')"],"metadata":{"id":"btUsDg8eAIOX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(EPOCHS):\n","  print('Epoch {} de {}'.format(epoch+1, EPOCHS))\n","  print('------------------')\n","  start_time = time.time() \n","\n","  train_acc, train_loss, train_f1, train_weight , history_train = train_model(\n","    model,train_data_loader, loss_fn, optimizer, device, epoch, len(train_data)\n","  )\n","  finish_time = time.time()\n","    \n","  validation_acc, validation_loss, validation_f1, val_weight, history_val = eval_model(\n","    model, validation_data_loader, loss_fn, device, len(val_data), 'Validación'\n","  )\n","  checkpoint = {'epoch': epoch + 1, 'state_dict': model.state_dict(),\n","                 'optimizer': optimizer.state_dict(),\n","                 'history_by_epoch': {\n","                 'train':history_train,\n","                 'val':history_val\n","                                  }\n","                }\n","  torch.save(checkpoint, f'yelp_CLEAN_checkpoint_{epoch+1}.pth')\n","  print('Entrenamiento: Loss: {}, accuracy: {}, f1_score: {}, f1_weight: {}'.format(train_loss, train_acc, train_f1, train_weight))\n","  print('Validación: Loss: {}, accuracy: {},  f1: {}, f1_weight: {}'.format(validation_loss, validation_acc, validation_f1, val_weight))\n","  print('')\n","  elapsed_time = finish_time - start_time\n","  with open (f'{NAME} LR {LEARNING_RATE}.txt','a') as f:\n","    f.write(f'''    name: {NAME_BERT_MODEL}\n","    epoch: {epoch+1}\n","    learning rate: {LEARNING_RATE}\n","    bach size: {BATCH_SIZE}\n","    max len: {MAX_LEN}\n","    n clases: {NCLASSES}\n","    dropout: {DropOut}\n","    Train:\n","        -loss: {train_loss}, Accuracy: {train_acc}, f1_score: {train_f1}, f1_weight: {train_weight}\n","    Validation\n","        -loss: {validation_loss}, Accuracy: {validation_acc}, f1_score: {validation_f1}, f1_weight: {val_weight}\n","    time: {time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}\n","\n","    ---------------------------------   \n","    \n","    ''')"],"metadata":{"id":"QcNZz3fIAKmu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ls"],"metadata":{"id":"W6ksgmDEBMjt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint = torch.load('epoch_0_bert_18000_weightModelBERT.pth')\n","checkpoint.keys()"],"metadata":{"id":"dpdEl0oWBD-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(checkpoint['weight_model'])"],"metadata":{"id":"Em90Vk-7BXxn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["validation_acc, validation_loss, validation_f1, val_weight, history_val = eval_model(\n","    model, test_data_loader, loss_fn, device, len(test_data), 'test'\n","  )"],"metadata":{"id":"Iz1Rd6HxPGgv"},"execution_count":null,"outputs":[]}]}