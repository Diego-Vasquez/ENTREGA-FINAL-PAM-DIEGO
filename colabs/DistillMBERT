{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DistillMBERT","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"SGkHplDmjeN-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd drive/MyDrive/univer/Expociencia/Code/distillmbert"],"metadata":{"id":"Tzx7CPJkqTfn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"IIqlSBhgZPO5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from transformers import  DistilBertModel, DistilBertTokenizerFast , DistilBertForMaskedLM\n","from torch.utils.data import Dataset, DataLoader\n","from torch import nn, optim\n","import torch\n","from sklearn.metrics import f1_score\n","import time\n","from sklearn.metrics import accuracy_score\n","from pathlib import Path\n","from torch.nn import functional as F"],"metadata":{"id":"-fp7ru6XZW0w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["name = \"yelp\""],"metadata":{"id":"WdyU7-t7gMEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train_data = pd.read_csv('train_'+name+'_clean.csv')\n","#val_data = pd.read_csv('val_'+name+'_clean.csv')\n","test_data = pd.read_csv('test_'+name+'_clean.csv')"],"metadata":{"id":"ecPrf3cfZafE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train_data['Score'] = train_data['Score'].apply(lambda x: x-1) \n","#val_data['Score'] = val_data['Score'].apply(lambda x: x-1) \n","print('ok')"],"metadata":{"id":"OUxMv8LS4tUf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data['Score'] = test_data['Score'].apply(lambda x: x-1) \n"],"metadata":{"id":"bkwjsHhg0WXA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_LEN = 128 #Fijo\n","BATCH_SIZE = 32\n","NCLASSES= 5 #Fijo\n","DropOut = 0.1\n","RANDOM_SEED = 42\n","EPOCHS= 1\n","LEARNING_RATE=2e-5 "],"metadata":{"id":"-LYTp_VzywF0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#NAME_BERT_MODEL=\"distilbert-base-es-cased\"\n","#NAME=\"distilbert-base-es-cased\"\n","#LEARNING_RATE=2e-5 ## <- hiperparametro mas sensible\n","#myfile = Path(f'{NAME} LR {LEARNING_RATE}.txt')\n","#myfile.touch(exist_ok=True)\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","device= torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"id":"Y-QXEi8eamop"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["distill_bert = DistilBertModel.from_pretrained(\"Geotrend/distilbert-base-es-cased\")\n","tokenizer = DistilBertTokenizerFast.from_pretrained(\"Geotrend/distilbert-base-es-cased\")"],"metadata":{"id":"jZkyQCokavdZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#distil_mlm = DistilBertForMaskedLM.from_pretrained('Geotrend/distilbert-base-es-cased')\n","#ckp = torch.load('Pretrined_distill_CLEAN_1.pth', map_location=device)\n","#distil_mlm.load_state_dict(ckp['state_dict'])\n","#distill_bert.load_state_dict(distil_mlm.distilbert.state_dict())"],"metadata":{"id":"uM0lOotO-rVF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DataModel(Dataset):\n","    def __init__(self, reviews, labels, tokenizer, max_len, mode_truncation):\n","        self.reviews = reviews\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.mode_truncation = mode_truncation\n","\n","    def __len__(self):\n","        return len(self.reviews)\n","    \n","    def __getitem__(self, item):\n","        review = str(self.reviews[item]) \n","        label = self.labels[item]\n","        tokens = self.tokenizer.tokenize(review) \n","        tokens = ['[CLS]'] + tokens + ['[SEP]']\n","        if len(tokens) < self.max_len:\n","            tokens = tokens + ['[PAD]' for item in range(self.max_len-len(tokens))]\n","        elif len(tokens) > self.max_len:\n","            if self.mode_truncation == \"head\":\n","                tokens = tokens[:self.max_len-1] + ['[SEP]']    \n","            elif self.mode_truncation == \"head+tail\":\n","                tokens = tokens[:65] + tokens[-63:]#int((self.max_len)/2)\n","            elif self.mode_truncation == \"tail\":\n","                tokens = ['[CLS]'] + tokens[-int((self.max_len)/2):]\n","        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n","        tokens_ids_tensor = torch.tensor(token_ids)\n","        attn_mask = (tokens_ids_tensor != 0).long()\n","        return{\n","            'review':review,\n","            'input_ids':tokens_ids_tensor.flatten(),\n","            'attention_mask':attn_mask.flatten(),\n","            'label':torch.tensor(label, dtype=torch.long)}\n","\n","def data_loader(df, tokenizer, max_len, batch_size, modo):\n","  dataset = DataModel(\n","    reviews=df.Text.to_numpy(),\n","    labels = df.Score.to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=max_len,\n","    mode_truncation = modo\n","  )\n","  return DataLoader(dataset, batch_size= BATCH_SIZE, num_workers=2)"],"metadata":{"id":"_L8HuG3ja2Wm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train_data_loader = data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE, \"head+tail\")\n","#validation_data_loader = data_loader(val_data, tokenizer, MAX_LEN, BATCH_SIZE, \"head+tail\")\n","test_data_loader = data_loader(test_data, tokenizer, MAX_LEN, BATCH_SIZE, \"head+tail\")"],"metadata":{"id":"9e_t7KAvbH96"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DistillBERTModel(nn.Module):\n","    def __init__(self,n_class):\n","        super(DistillBERTModel, self).__init__()\n","        self.num_labels = n_class\n","        self.distilbert = distill_bert\n","        self.classifier = nn.Linear(self.distilbert.config.hidden_size, self.num_labels)\n","        self.dropout = nn.Dropout(DropOut)\n","        nn.init.xavier_normal_(self.classifier.weight)\n","\n","    def forward(self, input_ids=None, attention_mask=None):\n","        distilbert_output = self.distilbert(input_ids=input_ids,\n","                                            attention_mask=attention_mask)\n","                                            #,return_dict=False)\n","        hidden_state = distilbert_output[0]                    \n","        pooled_output = hidden_state[:, 0]                   \n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output) \n","        return logits"],"metadata":{"id":"16xIDfgRbPFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = DistillBERTModel(NCLASSES)\n","model.to(device)\n","print('ok')"],"metadata":{"id":"aYgnKFgBbbr9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_fn = nn.CrossEntropyLoss().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"],"metadata":{"id":"RS0wf2YAbglN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model (model, data_loader, loss_fn, optimizer, device, epoch, n_examples):\n","  model=model.train()\n","  losses = []\n","  accuracy_global = []\n","  f1_score_global = []\n","  f1_weight_global = []\n","\n","  i = 0\n","  for batch in data_loader:\n","    input_ids = batch['input_ids'].to(device)\n","    attention_mask = batch['attention_mask'].to(device)\n","    labels = batch['label'].to(device)\n","    outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n","    _, preds = torch.max(outputs, dim=1)\n","    loss = loss_fn(outputs, labels)\n","    f1_score_global.append(np.mean(f1_score(preds.cpu().detach().numpy(), labels.cpu().detach().numpy(), average=None)))\n","    f1_weight_global.append(np.mean(f1_score(preds.cpu().detach().numpy(), labels.cpu().detach().numpy(), average='weighted')))\n","    ac = accuracy_score(preds.cpu().detach().numpy(), labels.cpu().detach().numpy())\n","    accuracy_global.append(ac)\n","    losses.append(loss.item())\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    if i%5==0:\n","        print('Ejemplo {}/{} , Entrenamiento: Loss: {}, accuracy: {} -- f1_score: {} -- f1 weighted: {}'.format(i, int(n_examples/BATCH_SIZE),\n","                                                                                               np.mean(losses), np.mean(accuracy_global), np.mean(f1_score_global), \n","                                                                                               np.mean(f1_weight_global)))\n","    if (i%1000==0 and i>9999) or (i == int(n_examples/BATCH_SIZE)-1):\n","        validation_acc, validation_loss, validation_f1, val_weight, history = eval_model(\n","            model, validation_data_loader, loss_fn, device, len(val_data), 'Validación'\n","        )\n","        print('Entrenamiento: Loss: {}, accuracy: {} -- f1_score: {} -- f1 weighted: {}'.format(np.mean(losses), np.mean(accuracy_global), np.mean(f1_score_global), \n","                                                                                                              np.mean(f1_weight_global)))\n","        print('Validación: Loss: {}, accuracy: {} -- f1_score: {} -- f1 weighted: {}'.format(validation_loss, validation_acc, validation_f1,\n","                                                                                                          val_weight\n","                                                                                                          ))\n","        weight = {f'weight_model':model.state_dict()}\n","        torch.save(weight, f'epoch_{epoch}_distilbert_{i}.pth')\n","        with open (f'{NAME} LR {LEARNING_RATE}.txt','a') as f:\n","                f.write(f'''    name: {NAME_BERT_MODEL}\n","                Iteracion : {i}\n","                Train:\n","                   -loss: {np.mean(losses)} , \n","                    Mean :Accuracy: {np.mean(accuracy_global)}, f1_score: {np.mean(f1_score_global)}, f1_micro: {np.mean(f1_weight_global)}\n","                    std : Accuracy: {np.std(accuracy_global)}, f1_score: {np.std(f1_score_global)}, f1_micro: {np.std(f1_weight_global)}\n","                    var : Accuracy: {np.std(accuracy_global)**2}, f1_score: {np.std(f1_score_global)**2}, f1_micro: {np.std(f1_weight_global)**2}\n","                Validation\n","                   -loss: {np.mean(history['loss'])},\n","                   -Mean  Accuracy: {np.mean(history['accuracy'])}, f1_score: {np.mean(history['f1_score'])}, f1_weighted: {np.mean(history['f1_weight'])}\n","                   -std:  Accuracy: {np.std(history['accuracy'])}, f1_score: {np.std(history['f1_score'])}, f1_weighted: {np.std(history['f1_weight'])}\n","                   -var:  Accuracy: {np.std(history['accuracy'])**2}, f1_score: {np.std(history['f1_score'])**2}, f1_weighted: {np.std(history['f1_weight'])**2}\n","                ---------------------------------    \n","    ''')\n","    i+=1\n","  return np.mean(accuracy_global), np.mean(losses), np.mean(f1_score_global), np.mean(f1_weight_global), {'loss':losses, \n","                                                                                                        'accuracy': accuracy_global,\n","                                                                                                        'f1_score': f1_score_global,\n","                                                                                                        'f1_weight':f1_weight_global}"],"metadata":{"id":"h_7QKP-sbi-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval_model(model, data_loader, loss_fn, device, n_examples, modo):\n","  model = model.eval()\n","  losses = []\n","  correct_predictions = 0\n","  accuracy_global = []\n","  f1_score_global = []\n","  f1_weighted_global = []\n","  i=0\n","  with torch.no_grad():\n","    for batch in data_loader:\n","      input_ids = batch['input_ids'].to(device)\n","      attention_mask = batch['attention_mask'].to(device)\n","      labels = batch['label'].to(device)\n","      outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n","      _, preds = torch.max(outputs, dim=1)\n","      loss = loss_fn(outputs, labels)\n","      f1_score_global.append(np.mean(f1_score(preds.cpu().detach().numpy(), labels.cpu().detach().numpy(), average=None)))\n","      f1_weighted_global.append(np.mean(f1_score(preds.cpu().detach().numpy(), labels.cpu().detach().numpy(), average='weighted')))\n","      ac = accuracy_score(preds.cpu().detach().numpy(), labels.cpu().detach().numpy())\n","      accuracy_global.append(ac)\n","      losses.append(loss.item())\n","      if i%5==0:\n","        print('Ejemplo {}/{} , {}: Loss: {}, accuracy: {} -- f1_score: {} -- f1 weighted: {}'.format(i, int(n_examples/BATCH_SIZE),\n","                                                                                               modo, np.mean(losses), np.mean(accuracy_global), np.mean(f1_score_global), \n","                                                                                               np.mean(f1_weighted_global)))\n","      i+=1\n","  return np.mean(accuracy_global), np.mean(losses), np.mean(f1_score_global), np.mean(f1_weighted_global),{'loss':losses, \n","                                                                                                            'accuracy': accuracy_global,\n","                                                                                                            'f1_score': f1_score_global,\n","                                                                                                            'f1_weight':f1_weighted_global}"],"metadata":{"id":"iP5kEKF5bnUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f'El modelo tiene {count_parameters(model)} de parámetros')"],"metadata":{"id":"emM7nvK8brTo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(EPOCHS):#5126\n","  print('Epoch {} de {}'.format(epoch+1, EPOCHS))\n","  print('------------------')\n","  start_time = time.time() \n","\n","  train_acc, train_loss, train_f1, train_weight , history_train = train_model(\n","    model, train_data_loader, loss_fn, optimizer, device, epoch, len(train_data)\n","  )\n","  finish_time = time.time()\n","  validation_acc, validation_loss, validation_f1, val_weight, history_val = eval_model(\n","    model, validation_data_loader, loss_fn, device, len(val_data), 'Validación'\n","  )\n","  checkpoint = {'epoch': epoch + 1, 'state_dict': model.state_dict(),\n","             'optimizer': optimizer.state_dict(),\n","             'history_by_epoch': {\n","                 'train':history_train,\n","                 'val':history_val\n","                                  }\n","                }  \n","\n","  torch.save(checkpoint, f'DistilBERT_checkpoint_{epoch+1}.pth')\n","  print('Entrenamiento: Loss: {}, accuracy: {}, f1_score: {}, f1_weight: {}'.format(train_loss, train_acc, train_f1, train_weight))\n","  print('Validación: Loss: {}, accuracy: {}, f1_score: {}, f1_weight: {}'.format(validation_loss, validation_acc, validation_f1, val_weight))\n","  print('')\n","  elapsed_time = finish_time - start_time\n","  with open (f'{NAME} LR {LEARNING_RATE}.txt','a') as f:\n","    f.write(f'''    name: {NAME_BERT_MODEL}\n","    epoch: {epoch+1}\n","    learning rate: {LEARNING_RATE}\n","    bach size: {BATCH_SIZE}\n","    max len: {MAX_LEN}\n","    n clases: {NCLASSES}\n","    dropout: {DropOut}\n","    Train:\n","      -loss: {np.mean(history_train['loss'])},\n","      -Mean  Accuracy: {np.mean(history_train['accuracy'])}, f1_score: {np.mean(history_train['f1_score'])}, f1_weighted: {np.mean(history_train['f1_weight'])}\n","      -std:  Accuracy: {np.std(history_train['accuracy'])}, f1_score: {np.std(history_train['f1_score'])}, f1_weighted: {np.std(history_train['f1_weight'])}\n","      -var:  Accuracy: {np.std(history_train['accuracy'])**2}, f1_score: {np.std(history_train['f1_score'])**2}, f1_weighted: {np.std(history_train['f1_weight'])**2}\n","    Validation\n","      -loss: {np.mean(history_val['loss'])},\n","      -Mean  Accuracy: {np.mean(history_val['accuracy'])}, f1_score: {np.mean(history_val['f1_score'])}, f1_weighted: {np.mean(history_val['f1_weight'])}\n","      -std:  Accuracy: {np.std(history_val['accuracy'])}, f1_score: {np.std(history_val['f1_score'])}, f1_weighted: {np.std(history_val['f1_weight'])}\n","      -var:  Accuracy: {np.std(history_val['accuracy'])**2}, f1_score: {np.std(history_val['f1_score'])**2}, f1_weighted: {np.std(history_val['f1_weight'])**2}\n","\n","    time: {time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}\n","\n","    ---------------------------------   \n","    \n","    ''')"],"metadata":{"id":"k_goGe41bveX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint = torch.load('epoch_0_distilbert_18000.pth')\n","checkpoint.keys()"],"metadata":{"id":"1IwPZ7swzcwl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(checkpoint['weight_model'])"],"metadata":{"id":"yHpD3uUaz-Rc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint['weight_model']"],"metadata":{"id":"btCsif5c1Vxw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["validation_acc, validation_loss, validation_f1, val_weight, history_val = eval_model(\n","    model, test_data_loader, loss_fn, device, len(test_data), 'test'\n","  )"],"metadata":{"id":"bIKdIlkrzW3E"},"execution_count":null,"outputs":[]}]}